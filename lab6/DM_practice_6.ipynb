{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i8adKFQS7E-"
   },
   "source": [
    "# **Лабораторна робота 6: Пошук аномалій та вирішення задачі *anomaly detection* за допомогою бібліотек `scikit-learn`та `PyTorch`**\n",
    "**Всі завдання виконуються індивідуально. Використання запозиченого коду буде оцінюватись в 0 балів.**\n",
    "\n",
    "**Лабораторні роботи де в коді буде використаня КИРИЛИЦІ будуть оцінюватись в 20 балів.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvneQUbQRRqZ"
   },
   "source": [
    "### Мета роботи:\n",
    "Ознайомитися з основними методами виявлення аномалій, навчитися використовувати бібліотеки `scikit-learn` та `PyTorch` для реалізації алгоритмів пошуку аномалій, проаналізувати ефективність різних методів на реальних наборах даних з Kaggle.\n",
    "\n",
    "\n",
    "### Опис завдання:\n",
    "\n",
    "1. **Постановка задачі**:\n",
    "   Використовуючи один із доступних наборів даних Kaggle (наприклад, *Credit Card Fraud Detection*, *Network Intrusion*, або інші), вам потрібно розв'язати задачу виявлення аномалій. Основна мета — ідентифікувати аномальні записи серед нормальних. Вибраний набір даних повинен містити мітки аномалій для перевірки результатів.\n",
    "\n",
    "2. **Етапи виконання завдання**:\n",
    "   - Завантажте та підготуйте набір даних.\n",
    "   - Проведіть попередню обробку даних (масштабування, заповнення пропущених значень, видалення нерелевантних ознак).\n",
    "   - Використайте різні методи виявлення аномалій:\n",
    "     - **Методи з бібліотеки scikit-learn**:\n",
    "       - Isolation Forest\n",
    "       - One-Class SVM\n",
    "       - Local Outlier Factor (LOF)\n",
    "     - **Методи з використанням PyTorch**:\n",
    "       - Автоенкодери для виявлення аномалій.\n",
    "   - Порівняйте отримані результати, обчисліть метрики якості (Precision, Recall, F1-Score).\n",
    "   - Оцініть, який метод найкраще підходить для вирішення задачі на вашому наборі даних.\n",
    "\n",
    "### Покрокова інструкція\n",
    "\n",
    "1. **Підготовка середовища**:\n",
    "   - Встановіть необхідні бібліотеки:\n",
    "     ```\n",
    "     pip install scikit-learn torch pandas numpy matplotlib\n",
    "     ```\n",
    "\n",
    "2. **Вибір набору даних з Kaggle**:\n",
    "   Зареєструйтесь на Kaggle та оберіть один із наборів даних для виявлення аномалій. Наприклад:\n",
    "   - [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
    "   - [Network Intrusion Detection](https://www.kaggle.com/xyuanh/benchmarking-datasets)\n",
    "\n",
    "3. **Попередня обробка даних**:\n",
    "   - Завантажте дані та проведіть їхню початкову обробку.\n",
    "   - Масштабуйте ознаки за допомогою `StandardScaler` або `MinMaxScaler`.\n",
    "   - Розділіть дані на навчальну і тестову вибірки.\n",
    "\n",
    "4. **Методи з бібліотеки `scikit-learn`**:\n",
    "\n",
    "   - **Isolation Forest**:\n",
    "     ```\n",
    "     from sklearn.ensemble import IsolationForest\n",
    "     ```\n",
    "\n",
    "   - **One-Class SVM**:\n",
    "     ```\n",
    "     from sklearn.svm import OneClassSVM\n",
    "     ```\n",
    "\n",
    "   - **Local Outlier Factor**:\n",
    "     ```\n",
    "     from sklearn.neighbors import LocalOutlierFactor\n",
    "     ```\n",
    "\n",
    "5. **Методи на основі нейронних мереж (PyTorch)**:\n",
    "\n",
    "   Використайте автоенкодер для пошуку аномалій. Побудуйте нейронну мережу з енкодером і декодером. Під час навчання порівняйте відновлені дані з вхідними та обчисліть помилку. Записи з великою помилкою можуть бути аномаліями.\n",
    "\n",
    "   - **Реалізація автоенкодера**:\n",
    "     ```\n",
    "     import torch\n",
    "     import torch.nn as nn\n",
    "     import torch.optim as optim\n",
    "     ```\n",
    "\n",
    "6. **Оцінка результатів**:\n",
    "   Використовуйте метрики оцінки якості:\n",
    "   - `Precision`, `Recall`, `F1-score`\n",
    "   ```\n",
    "   from sklearn.metrics import classification_report\n",
    "   ```\n",
    "\n",
    "7. **Звіт**:\n",
    "   - Поясніть, який метод дав найкращі результати.\n",
    "   - Проаналізуйте, чому деякі методи працюють краще на вашому наборі даних.\n",
    "   - Оцініть можливості використання глибоких нейронних мереж (автоенкодерів) для вирішення задачі.\n",
    "\n",
    "\n",
    "### Результати, які необхідно надати:\n",
    "1. Код рішення у вигляді Jupyter Notebook з аналізом результатів та поясненнями.\n",
    "\n",
    "\n",
    "### Дедлайн:\n",
    "[23 жовтня 23:59]\n",
    "\n",
    "\n",
    "### Корисні ресурси:\n",
    "- [Документація PyTorch](https://pytorch.org/docs/stable/index.html)\n",
    "- [Документація scikit-learn](https://scikit-learn.org/stable/documentation.html)\n",
    "- [Kaggle Datasets](https://www.kaggle.com/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NeqgMqm2UETO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n",
      "Class\n",
      "0    284315\n",
      "1       492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('creditcard.csv')\n",
    "print(data.info())\n",
    "print(data['Class'].value_counts()) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набір даних Credit Card Fraud Detection має дуже дисбалансований розподіл: серед 284,807 записів лише 492 є аномальними, решта — нормальні транзакції. Це ускладнює задачу, оскільки модель може легко досягати високої точності, передбачаючи більшість записів як нормальні."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isolation Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     56864\n",
      "           1       0.11      0.61      0.18        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.55      0.80      0.59     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n",
      "One-Class SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     56864\n",
      "           1       0.10      0.59      0.17        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.55      0.79      0.58     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n",
      "Local Outlier Factor\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     56864\n",
      "           1       0.01      0.07      0.02        98\n",
      "\n",
      "    accuracy                           0.99     56962\n",
      "   macro avg       0.51      0.53      0.51     56962\n",
      "weighted avg       1.00      0.99      0.99     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
    "y_pred_if = iso_forest.fit_predict(X_test)\n",
    "y_pred_if = [1 if x == -1 else 0 for x in y_pred_if]\n",
    "\n",
    "oc_svm = OneClassSVM(kernel='rbf', gamma=0.001, nu=0.01)\n",
    "y_pred_svm = oc_svm.fit_predict(X_test)\n",
    "y_pred_svm = [1 if x == -1 else 0 for x in y_pred_svm]\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
    "y_pred_lof = lof.fit_predict(X_test)\n",
    "y_pred_lof = [1 if x == -1 else 0 for x in y_pred_lof]\n",
    "\n",
    "print(\"Isolation Forest\")\n",
    "print(classification_report(y_test, y_pred_if))\n",
    "print(\"One-Class SVM\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"Local Outlier Factor\")\n",
    "print(classification_report(y_test, y_pred_lof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.6431\n",
      "Epoch 2/20, Loss: 0.5876\n",
      "Epoch 3/20, Loss: 0.5700\n",
      "Epoch 4/20, Loss: 0.5618\n",
      "Epoch 5/20, Loss: 0.5519\n",
      "Epoch 6/20, Loss: 0.5471\n",
      "Epoch 7/20, Loss: 0.5432\n",
      "Epoch 8/20, Loss: 0.5418\n",
      "Epoch 9/20, Loss: 0.5409\n",
      "Epoch 10/20, Loss: 0.5416\n",
      "Epoch 11/20, Loss: 0.5392\n",
      "Epoch 12/20, Loss: 0.5389\n",
      "Epoch 13/20, Loss: 0.5383\n",
      "Epoch 14/20, Loss: 0.5427\n",
      "Epoch 15/20, Loss: 0.5381\n",
      "Epoch 16/20, Loss: 0.5377\n",
      "Epoch 17/20, Loss: 0.5376\n",
      "Epoch 18/20, Loss: 0.5375\n",
      "Epoch 19/20, Loss: 0.5377\n",
      "Epoch 20/20, Loss: 0.5374\n",
      "Autoencoder\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98     56864\n",
      "           1       0.03      0.91      0.06        98\n",
      "\n",
      "    accuracy                           0.95     56962\n",
      "   macro avg       0.52      0.93      0.52     56962\n",
      "weighted avg       1.00      0.95      0.97     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, X_train.shape[1]),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train[y_train == 0], dtype=torch.float32) \n",
    "\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, len(X_train_tensor), batch_size):\n",
    "        batch = X_train_tensor[i:i+batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(X_test_tensor)\n",
    "    reconstruction_errors = torch.mean((reconstructions - X_test_tensor) ** 2, dim=1)\n",
    "\n",
    "\n",
    "threshold = np.percentile(reconstruction_errors.numpy(), 95)  \n",
    "\n",
    "y_pred_autoencoder = (reconstruction_errors.numpy() > threshold).astype(int)\n",
    "\n",
    "print(\"Autoencoder\")\n",
    "print(classification_report(y_test, y_pred_autoencoder))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation Forest:\n",
    "\n",
    "Precision для аномалій становить 11%, що свідчить про великий відсоток помилкових спрацьовувань. Однак, recall для аномалій дорівнює 61%, що вказує на здатність методу виявляти аномальні записи, хоча й не з дуже високою точністю.\n",
    "F1-score для аномалій становить 18%, що свідчить про середню ефективність. Метод здатний знайти частину аномалій, але й має помилки.\n",
    "\n",
    "One-Class SVM:\n",
    "\n",
    "Показав схожі результати до Isolation Forest тільки на пару сотих відсотків нижчу точність.\n",
    "\n",
    "Local Outlier Factor:\n",
    "\n",
    "Показав погані результати, що свідчить, що цей метод поганий для поставленої задачі\n",
    "\n",
    "Автоенкодер:\n",
    "\n",
    " виявив 91% з 98 аномалій, проте велику частину нормальних записів класифікував як аномалії, що призвело до низької precision.\n",
    "\n",
    " Автоенкодери показали результати, що свідчать про їх обмежену ефективність у даному випадку. Хоча вони виявилися здатними досягти високого значення recall (91%) для аномалій, їх precision (3%) був дуже низьким. Це означає, що більшість з того, що автоенкодер вважає аномаліями, насправді є нормальними записами, що призводить до високого рівня помилкових спрацьовувань. Висока помилка відновлення свідчить про те, що автоенкодер не здатний адекватно відтворювати дані аномалій, через що він не може бути надійним для задачі виявлення аномалій у такому дисбалансному наборі.\n",
    "\n",
    " На мою думку найкращим варіантом для цієї задачі являється Isolation Forest, хоча він і має невисоку точність recall на рівні 61% але має precision 11%, що робить його більш заблансованим для цього набору даних"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
